---
title: "pstat 131 hw 6"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
library(tidymodels)
library(tidyverse)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(corrplot)
library(klaR) 
library(pROC)
library(glmnet)
library(dplyr)
library(janitor)
tidymodels_prefer()
library(randomForest)
library(rpart)
library(rpart.plot)
library(ranger)
library(vip)
```
1. Read in the data and set things up as in Homework 5:
```{r}
pokemon<- read.csv('/Users/ankitapattnaik/Downloads/homework-5/data/Pokemon.csv')
```
Use clean_names()
Filter out the rarer Pokémon types
Convert type_1 and legendary to factors
Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

Fold the training set using v-fold cross-validation, with v = 5. Stratify on the outcome variable.

Set up a recipe to predict type_1 with legendary, generation, sp_atk, attack, speed, defense, hp, and sp_def:

Dummy-code legendary and generation;
Center and scale all predictors.

```{r}
pokemon <-clean_names(pokemon)
set.seed(0714)

pokemon <- pokemon %>%
  filter(type_1 == "Bug" | type_1 == "Fire" | type_1 == "Grass" | type_1 ==  "Normal" | type_1 ==  "Water" | type_1 == "Psychic")

pokemon$type_1 <- as.factor(pokemon$type_1)
pokemon$legendary <- as.factor(pokemon$legendary)
pokemon$generation <- as.factor(pokemon$generation)


pokemon_split <- initial_split(pokemon, strata = type_1, prop = 0.7)
pokemon_split
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

dim(pokemon_train)
dim(pokemon_test)
pokemon_fold <- vfold_cv(pokemon_train, strata = type_1, v = 5)

pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk +
                           attack + speed + defense + hp + sp_def,
                         data = pokemon_train) %>%
  step_dummy(c(legendary, generation)) %>%
  step_normalize(all_predictors())

```





2. Create a correlation matrix of the training set, using the corrplot package. Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).

```{r}
pokemon_train2<-pokemon_train[,sapply(pokemon_train,is.numeric)]
pokemon_train2 %>%
  cor() %>%
  corrplot(type = 'lower', diag = FALSE,
           method = 'color',addCoef.col = 'Black')
```

What relationships, if any, do you notice? Do these relationships make sense to you?

They all seem to have low correlation which makes no sense to me. I got rid of the non-numeric variables / non continuous by creating a pokemon_train2 with only numeric values. 





3. First, set up a decision tree model and workflow. Tune the cost_complexity hyperparameter. Use the same levels we used in Lab 7 – that is, range = c(-3, -1). Specify that the metric we want to optimize is roc_auc.

Print an autoplot() of the results. 
```{r,cache = TRUE}
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(pokemon_recipe)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf,
  resamples = pokemon_fold,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)
autoplot(tune_res)
```

What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

A single decision tree performs better with a smaller complexity penalty, in this case .001 and .005. It then drops.





4. What is the roc_auc of your best-performing pruned decision tree on the folds? Hint: Use collect_metrics() and arrange().

```{r}
collect_metrics(tune_res)%>%
  arrange(-mean)
```

The roc_auc is best performining at 0.64.




5. Using rpart.plot, fit and visualize your best-performing pruned decision tree with the training set.

```{r,cache = TRUE}
best_complexity <- select_best(tune_res)
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)
class_tree_final_fit <- fit(class_tree_final, data = pokemon_train)
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

Now set up a random forest model and workflow. Use the ranger engine and set importance = "impurity". Tune mtry, trees, and min_n. Using the documentation for rand_forest(), explain in your own words what each of these hyperparameters represent.

```{r}
rf_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
rf_wf <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(pokemon_recipe)
```

mtry: represents the number of predictors that are sampled in the split
trees: the total trees
min_n: minimum number of observations before splitting the nodes


Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that mtry should not be smaller than 1 or larger than 8. Explain why not. What type of model would mtry = 8 represent?

```{r}
param_grid2 <- grid_regular(mtry(range = c(1, 8)),trees(range = c(50,300)),min_n(range=c(1,5)), levels = 8)
```

mtry should not be smaller than 1 or larger than 8 because then it would become a bagging model. mtry=8 would make it a bagging model. 




6. Specify roc_auc as a metric. Tune the model and print an autoplot() of the results. 

```{r,cache = TRUE}
tune_res2 <- tune_grid(
  rf_wf,
  resamples = pokemon_fold,
  grid = param_grid2,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res2)
```

What do you observe? What values of the hyperparameters seem to yield the best performance?

The hyperparameters closer to 300 seem to yield the best result. The higher the hyperparameters, the more accurate. 





7. What is the roc_auc of your best-performing random forest model on the folds? Hint: Use collect_metrics() and arrange().

```{r}
 collect_metrics(tune_res2)%>%
  arrange(-mean)
```

The roc_auc is 0.757. 





8. Create a variable importance plot, using vip(), with your best-performing random forest model fit on the training set.

```{r}
best_complexity2 <- select_best(tune_res2)
rf_final <- finalize_workflow(rf_wf, best_complexity2)
rf_final_fit <- fit(rf_final, data = pokemon_train)
rf_final_fit %>%
  extract_fit_engine() %>%
  vip()
```

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

sp_atk, attack, speed, hp, sp_def, and defense seem  to be the most useful. generation_X5, legendary_True, generation_X4, and generation_X2 seem the least useful. I would say these results are expected. 



9. Finally, set up a boosted tree model and workflow. Use the xgboost engine. Tune trees. Create a regular grid with 10 levels; let trees range from 10 to 2000. Specify roc_auc and again print an autoplot() of the results.

```{r, cache=TRUE}
bt_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")
bt_wf <- workflow() %>%
  add_model(bt_spec %>% set_args(trees = tune())) %>%
  add_recipe(pokemon_recipe)
param_grid3 <- grid_regular(trees(range = c(10,2000)), levels = 10)
tune_res3 <- tune_grid(
  bt_wf,
  resamples = pokemon_fold,
  grid = param_grid3,
  metrics = metric_set(roc_auc)
)
autoplot(tune_res3)
```

What do you observe?

While they fluctuate, the range of the roc_auc only differ by a little over 0.005. This implies that you don't really need that many trees to yield your results. 

What is the roc_auc of your best-performing boosted tree model on the folds? Hint: Use collect_metrics() and arrange().

```{r}
collect_metrics(tune_res3)%>%
  arrange(-mean)
```
The roc_auc of the best performing boosted tree model is 0.733.





10. Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use select_best(), finalize_workflow(), and fit() to fit it to the testing set.


```{r}
a<-collect_metrics(tune_res)%>%
  arrange(-mean)%>%
  filter(row_number()==1)
a['type']<-'pt'
b<-collect_metrics(tune_res2)%>%
  arrange(-mean)%>%
  filter(row_number()==1)
b['type']<-'rf'
c<-collect_metrics(tune_res3)%>%
  arrange(-mean)%>%
  filter(row_number()==1)
c['type']<-'bt'
inner1<-rbind(a[2:8],b[4:10])
inner2<-rbind(inner1,c[2:8])
inner2
```
Random forest works best.

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

```{r}
best_complexity2 <- select_best(tune_res2)
rf_final2 <- finalize_workflow(rf_wf, best_complexity2)
rf_final_fit2 <- fit(rf_final, data = pokemon_test)
augment(rf_final_fit2, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
 augment(rf_final_fit2, new_data = pokemon_test) %>%
  roc_curve(type_1,.pred_Bug,.pred_Fire,.pred_Grass,.pred_Normal,
            .pred_Psychic,.pred_Water) %>%
  autoplot()
 augment(rf_final_fit2, new_data = pokemon_test) %>%
  roc_auc(type_1,.pred_Bug,.pred_Fire,.pred_Grass,.pred_Normal,.pred_Psychic,.pred_Water)
```

Which classes was your model most accurate at predicting? Which was it worst at?

They were all fit to the testing set so they are all accurate. 
